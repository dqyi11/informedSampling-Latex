%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

\makeatletter
\let\NAT@parse\undefined
\makeatother

\usepackage{xspace}
\usepackage{amsmath,amssymb,amsfonts,amsfonts}
\usepackage{algorithm}
\usepackage[noend]{algorithmic}
\usepackage{url}
\usepackage{bm}
\usepackage{graphicx,subfigure}
\usepackage{color}
\usepackage{hyperref}
%\hypersetup{bookmarksopen,
%bookmarksnumbered,
%pdfpagemode=UseOutlines,
%colorlinks=true,
%linkcolor=blue,
%anchorcolor=blue,
%citecolor=blue,
%filecolor=blue,
%menucolor=blue,
%urlcolor=blue
%}


\title{\LARGE \bf
Informed sampling by Markov Chain Monte Carlo methods
}


\author{
Rohan Thakker$^{*}$,
Cole Gulino$^{*}$,
Daqing Yi$^{}$,
Oren Salzman$^{}$ and
Siddhartha Srinivasa$^{}$% <-this % stops a space
\thanks{*R. Thakker and C. Gulino contributed equally to this paper.}
\thanks{This work was (partially) funded by the National Science Foundation IIS (\#1409003), Toyota Motor Engineering \& Manufacturing (TEMA), and the Office of Naval Research.}% <-this % stops a space
\thanks{$^{1}${\tt\small \{rthakker, cgulino, daqingy, osalzman, ss5\} @andrew.cmu.edu}}%
%
\\        
Robotics Institute, Carnegie Mellon University$^{1}$
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  macros 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{macros.tex}

\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}


Sampling-based motion-planning algorithms~\cite{CBHKKLT05, L06} have proven to be an effective tool at solving motion-planning problems.
They search through a continuous state space~$\calX$ by sampling random states and maintaining a discrete graph~$G$ called a \emph{roadmap}.
Vertices and edges in $G$ correspond to collision-free states and paths, respectively.

Roughly speaking, these algorithms iteratively sample new states.
This is required to ensure that, as the number of samples tends to infinity, 
(i)~a solution will be found 
(a property called \emph{probabilistic completeness})
and that
(ii)~given some optimization criteria, the quality of the solution will progressively converge to the quality of the optimal solution
(a property called \emph{asymptotic  optimality}).

Initially, 
when no path has yet to be found, 
the samples need to be drawn from the entire state space~$\calX$.
However, once a path $\gamma$ is produced,  algorithms that seek \emph{high-quality paths} can limit their sampling domain to a subset of~$\calX$ only  containing states that may produce higher-quality paths than~$\gamma$.
Following Gammell et al.~\cite{GSB14}, we call this subset the \emph{informed subset} and denote it~$\Cinf$.
\textbf{In this work we address the problem of efficiently producing samples in the informed subset for systems with differential constraints}. 

For Euclidean spaces optimizing for path length, 
\Cinf can be analytically expresses as a prolate hyperspheroid and can be sampled directly~\cite{GSB14}.
Indeed, directly sampling in \Cinf has been shown to dramatically improve computation time when compared to sampling in $\calX$, especially in high dimensions. 

Unfortunately, in more general settings such as
for systems with differential constraints,
it is not clear how to directly sample \Cinf.
%
One approach to produce samples in \Cinf is via \emph{rejection sampling}---sampling a state $x \in \calX$ and then testing if $x \in \Cinf$.
However, when the size of the informed space~$\Cinf$ is much smaller than entire state space~$\calX$, then this procedure may dominate the running time of the algorithm~\cite{KTC16}.
Recently, Kunz et al.~\cite{KTC16} showed, under some technical assumptions, how to efficiently produce samples in~$\Cinf$ by \emph{Hierarchical rejection sampling} (HRS). 
Here, individual dimensions are sampled recursively 
and then combined. Rejection sampling is performed for these partial samples until a suitable sample has been produced. 
%The novelty of HRS is the ability to fail fast. 

Unfortunately, both rejection sampling and HRS may produce a large number of rejected samples especially in high-dimensional spaces.
This may cause the search algorithm to spend most of its time trying to produces samples in~$\Cinf$ rather than explore it.
In this paper, we suggest an alternative approach to produce samples in the informed set \Cinf for a wide range of settings.
\textbf{Our main insight is to formulate the problem as the problem of sampling uniformly below a level set of an implicit function and then apply optimization and statistical tools.}
Specifically, our approach consists of two stages:
in the first, a random sample $x \in \calX$ is retracted to the boundary of~$\Cinf$ by running a gradient-descent algorithm;
in the second stage, this retracted sample~$\tilde{x}$ is used to seed a Monte Carlo sampling chain which allows us to  produce samples that (approximately) cover~$\Cinf$  uniformly.

Our approach requires that the system has a solution to the two-point boundary value problem (2pBVP)~\cite{L06, H02} and that a gradient can be defined over the cost function.
Indeed, we demonstrate the efficiency of our approach on a wide variety of systems and show that it enables reducing the planning time by several orders of magnitute when compared to algorithms using rejection sampling or HRS.

%While, original sampling-based algorithms such as RRT~\cite{LK01} and PRM~\cite{KSLO96} only guaranteed to asymptotically return \emph{a} solution, if one exists, they did not provide any guarantees on the \emph{quality} of the solution, given some optimization criteria.
%Karaman and Frazzoli~\cite{KF11}, presented variants of PRM and RRT, named PRM* and RRT*, respectively that were shown to produce paths who's cost converges asymptotically to the minimal-cost path.
%Additional algorithms followed, increasing the converges rate by various techniques such as lazy dynamic programming~\cite{GSB15, JSCP15}, relaxing optimality to near-optimality~\cite{DB14, SH16} and more.
%
%However, recent years 
%Original planners such as RRT~\cite{LK01} and PRM~\cite{KSLO96} did not provide any guarantee on the \emph{s}


\section{Related work}
\label{sec:related_work}
We start in Sec.~\ref{subsec:planning} by giving an overview of relevant sampling-based motion-planning algorithms.
We then continue in Sec.~\ref{subsec:sampling} to describe different approaches used to sample~$\calX$.
We conclude our literature review in Sec.~\ref{subsec:mcmc} with a brief overview of Markov Chain Monte Carlo methods.

\subsection{Planning}
\label{subsec:planning}
Initial sampling-based algorithms such as RRT~\cite{LK01} and PRM~\cite{KSLO96} did not take into account the \emph{quality} of a path, given some optimization criteria, and only guaranteed to asymptotically return \emph{a} solution, if one exists.
Karaman and Frazzoli~\cite{KF11}, presented variants of PRM and RRT, named PRM* and RRT*, respectively that were shown to produce paths who's cost converges asymptotically to the minimal-cost path.
This was done by recognizing the underlying connections between stochastic sampling-based motion planning and the theory of random geometric graphs (see also~\cite{SSH16}).
Additional algorithms followed, increasing the converges rate by using various techniques such as lazy dynamic programming~\cite{GSB15, JSCP15}, relaxing optimality to near-optimality~\cite{DB14, SH16} and more.


Many of the algorithms mentioned require solving a two-point boundary value problem (2pBVP) to perform exact and optimal connections between vertices in the roadmap.
For holonomic robots, these are simply straight lines in the state space, but for kinodynamic sytems with arbitrary cost functions,  computing an optimal trajectory between two states is non-trivial in general.

Xie et al.~\cite{XBPA15} use a variant of sequential quadratic programming (SQP) to solve 2pBVP and integrate it with BIT*~\cite{GSB15}.
Webb and van den Berg~\cite{WB13} use a fixed-final-state-free-final-time controller to solve the 2pBVP  with respect to a cost function that allows for balancing between the duration of the trajectory and the expended control effort.
Perez et al.~\cite{PPKKL12} propose a variant of RRT* that automatically defines a distance metric and node extension method by locally linearizing
the domain dynamics and applying linear quadratic regulation (LQR).

%Finally, we note that there have been many papers presenting solutions to the 2pBVP for specific systems such as
%
%Jeong Hwan Jeon, Raghvendra V. Cowlagi, Steven C. Peters, Sertac Karaman, Emilio Frazzoli, Panagiotis Tsiotras, Karl Iagnemma: Optimal motion planning with the half-car dynamical model for autonomous high-speed driving. ACC 2013: 188-193
%Dubins cars optimizng for both time and energy (RSS submission 107 by Shammas, Elie) 

%possible additional papers on Kinodynamic RRT*:~\cite{KF10, GPPK13}

\subsection{Sampling}
\label{subsec:sampling}
There is a rich body of literature on how to produce samples that increase the efficiency of the planner in terms of finding a solution or producing high-quality solutions.
Heuristic approaches include
sampling on the medial axis~\cite{WAS99a, WAS99b, LTA03, YDLTA14},
sampling near the boundary of the obstacles~\cite{ABDJV98, YTEA12},
resampling along a given trajectory~\cite{APD11, AS11}
and more~\cite{US03, SWT09}

Of specific interest to our work are approaches that produce samples in the informed set~\Cinf.
As mentioned in Sec.~\ref{sec:intro} Gammel et al.~\cite{GSB14} describe an approach to sample uniformly in~\Cinf for the specific case of where $\calX = \R^d$ and when optimizing for path length.
To the best of our knowledge, the only method to produce samples in non-Euclidean spaces (other than rejection sampling) is HRS by Kunz et al.~\cite{KTC16}.

\subsection{MCMC}
\label{subsec:mcmc}

An alternative sampling framework, which is widely used in statistical machine learning, is Monte Carlo Simulation~\cite{M97}.
It uses the numerical experimental power of computers to generate samples toward a desired distribution $ \pi (x) $.
The generated set of samples are a discrete approximation $ \hat{\pi} (x) $ of the desired distribution $ \pi (x) $.
A approximated desired distribution $ \hat{\pi} (x) $ can be used in any problem that states are modeled random variable, e.g. motion tracking~\cite{KBD04}, data regression~\cite{TL11} and state estimation~\cite{ASC13}.

Markov Chain Monte Carlo (MCMC)~\cite{ADDJ03} is one most popular class that constructs a Markov chain of random transition that moves samples from an original proposed distribution to a desired distribution $ \pi (x) $.
Unlike a rejection sampling, MCMC does a random walk in a parametric space and concentrates on an important area.
Though the convergence to the desired distribution is proved to be guaranteed, the nature of random walk determines that the convergence of MCMC can be slow in a high-dimensional space.
Hamiltonian Monte Carlo(HMC)~\cite{N11} is thus introduced to reduce the random walk but directly moving to the target by heuristic.

HMC adopts physical system dynamics to propose future states in the Markov chain~\cite{N11}.
It is named ``Hamiltonian, because the concept origins from Hamiltonian dynamics, which associates the system energy with the resulting dynamics.
Define the system dynamics of an object as the position $ x $ and the momentum $ q $ over time $ t $.
We can have a system energy $ H $ of the object that consists of kinetic energy $ K $ and potential energy $ U $, which is written as $ H(x,q) = U(x) + K(q) $.
The kinetic energy $ K(q) $ is determined by the momentum $ q $ of the object, while the potential energy $ U(x) $ is determined by the position $ x $ of the object.
The Hamiltonian dynamics is determined by the gradients of the system energy, which is
$ \frac{dx}{dt} = \frac{\partial H}{\partial q} $ and $ \frac{dq}{dt} = - \frac{\partial H}{\partial x} $.
In HMC, the state to be sampled is used as the position $ x $, and the associated probability $ P(x) $ is defined as the kinematic energy $ K(x) $.
An auxiliary variable is used as the momentum $ q $, which is associated with another probability that is defined as the potential energy $ U(q) $.
Thus, the system energy $ H(x, q) $ is associated with a joint probability $ P(x, q) $. 
As we are interested with sampling from marginal probability $ P(x) $ is interested, we will sample $ [ x ,q ] $ from $ P(x, q) $ and discard the momentum $ q $ as an indirect way of sampling.

The canonical distribution is often choose to bridge a system energy with a probability distribution, which is written as $ P(x, q) = \frac{1}{Z} \exp ( -H(x,q) / T ) $.
If two different states have the same energy, the probabilities of them are equivalent.
Given a target distribution $ \pi(x) $, the potential energy is defined in the form $  U(x) = - \log ( \pi(x) T ) $.

If we have of state $ x $ and define a potential energy , we can have the canonical distribution equal to $ \pi(x) $.
The collected set of $ x $ is equivalent to sampling from $ \pi(x) $.
Thus, we can use simulating the Hamiltonian dynamics to generate a set of samples from a target distribution $ \pi (x) $.
The properties of Hamiltonian dynamics guarantees so that the proposals will always be accepted, which accelerates the convergence rate.

We are going to integrate this idea into our sampling framework of efficiently generating samples under kinetic constraints. 
We define a target distribution as a uniform distribution subject to kinetic constraints and cost budgets.

\section{Motivating example---Minimal-Time Double Integrator}

We start by considering a simple yet important dynamical system---the double integrator. 
We are given a one-dimensional point robot with bounded velocity and bounded acceleration moving amid obstacles. We wish to compute the minimal-time trajectory between two states $x_s, x_g$.
A state in this model is described by 
the position $q \in \R$
and
the velocity $\dot{q}\in \R$ of the position
%each coordinate axis.
The dynamics of the system is described by:
$$
x = (q, \dot{q}); 
\hspace{5mm}
u = \ddot{q}.
$$
Here, the control $u \in [u_{\text{min}}, u_{\text{max}}]$ is the (bounded) acceleration. 


Notice that 
(i)~this model can be seen as a simplified one-dimensional instance of a robot manipulator with many degrees of freedom and that
(ii)~closed form solutions exist to the 2pBVP for this specific case (as well as the generalized instance)~\cite{HN10}.

Let $T(x_1, x_2)$ denote the minimal time to move the robot from state $x_1$ to $x_2$.
Furthermore, set
$$
T(x) = T(x_s, x) + T(x, x_g).
$$ 
Namely, $T(x)$ is the minimal time to reach the goal from the start subject to passing through the state $x$.

\subsection{Informed set}
Recall that for Euclidean spaces minimizing path length, the informed set is a prolate hyperspheroid.
Moreover, this ellipsoid is defined only be the cost $\hat{c}$ of the current best solution and not by the location of the start and goal.

For the case of a double integrator, this is not the case:
\os{show the explicit math and that it depends on the location of the start and goal}

See Fig.~\ref{fig:informed_1d_di}.
\begin{figure}[tb]
  \centering
  	\includegraphics[height = 4.cm ]{level_set.jpg}
  \caption{
    \captionstyle
  	Level sets for the case of minimal-time double integrator. A point $x = (q, \dot{q})$  denote the position and velocity of the robot while the color at $x$ denotes the cost to reach $x_t$ from $x_s$ subject to passing through the state $x$.
  	}
   	\label{fig:informed_1d_di}
	\vspace{-5.5mm}
\end{figure}



\section{Algorithm}
\label{sec:algorithm}

Define the dynamics of a robot as 
\begin{equation}
\label{eq:kinematics}
\dot{\bm{x}}(t) = f( \bm{x}(t) , \bm{u}(t) ), 
\end{equation}
in which $ \bm{x} \in \mathcal{X} \subseteq \mathbb{R}^n $ is the state of the robot, and $ \bm{u} \in \mathcal{U} \subseteq \mathbb{R}^m $ is the control input.
The dynamics determines a state trajectory $ \pi $, given a duration $ T $, an initial state $ \bm{x}(0) $ and an initial input $ \bm{u}(0) $.
The cost of a trajectory $ \pi $ is defined as the accumulated cost along the path, which is 
\begin{equation}
\label{eq:path_cost}
c(\pi) = \int_0^{T} c( x(t), u(t) ) dt.
\end{equation}
Consider the obstacles in a workspace, we have $ \mathcal{X}_{free} \subseteq \mathcal{X} $.
Consider the bounds of the control input, we have $ \mathcal{U}_{free} \subseteq \mathcal{U} $.
The dynamics of the robot in Equation~\eqref{eq:kinematics}, free state space $  \mathcal{X}_{free} $ and free control input space $ \mathcal{U}_{free} $ determines a free trajectory space $ \Pi_{free} $, which contains all feasible trajectories $ \pi \in \Pi_{free} $.

We can define an optimal kinodynamic path-planning problem that finds a kinodynamic path minimizing the cost in Definition \ref{defn:optimal_kino_path_planning}.
\begin{defn}{ \textbf{Optimal Kinodynamic Path-Planning} }
\label{defn:optimal_kino_path_planning}
	Find an optimal path $  \pi^* $ that satisfies
	\begin{itemize}
	\item $\forall t \in [0,T] $, $ \bm{x} (t) \in \mathcal{X}_{free}  $ and $ \bm{u} (t) \in \mathcal{U}_{free} $;
	\item $ \bm{x} (0) = x_{init} $, $ \bm{x} (T) = x_{goal} $, $ \bm{u} (0) = u_{init} $ and $ \bm{u} (T) = u_{goal} $; and
	\item $ \pi^* = \max_{ \pi \in \Pi_{free} } c( \pi ) $. 
	\end{itemize}
\end{defn}

\dy{Do we want to set an initial and a goal for the control input? Does this simplify the problem solving?}

\begin{algorithm}
	\begin{algorithmic}[1]
		\STATE
   	\end{algorithmic}
	\caption{MCMC Informed Sampling}
	\label{alg:mcmc_informed_sampling}
\end{algorithm}

We 



\section{Results}

\subsection{Uniformity}

(Give a set of parameters)
We firstly need to propose a way of measuring uniformity.
Then we can apply it to compare MCMC approach with rejection sampling and hierarchical rejection sampling.
We are interested with
\begin{itemize}
	\item asymptotic behavior (assumption: MCMC converge to a close uniformity)
	\item convergence rate (assumption: MCMC converge faster)
\end{itemize}

\subsection{Efficiency}

The efficiency is shown by how the time is needed when the number of nodes are increased.
(Y: time spent) and (X: number of nodes).

\subsection{Integration}

We're going to test with RRT*, which falls into a informed RRT*.

We are going to show how the solution cost falls along time.
(Y: solution cost) and (X: time spent).

\begin{itemize}
\item DIMT (double integrator minimum time)
\item Dubins car
\item Airplane
\end{itemize}

\section{Conclusion}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliography{bibliography}
\bibliographystyle{IEEEtran}
%\bibliography{IEEEabrv,bibliography}


\end{document}
