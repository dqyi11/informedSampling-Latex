%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

\makeatletter
\let\NAT@parse\undefined
\makeatother

\usepackage{xspace}
\usepackage{amsmath,amssymb,amsfonts,amsfonts}
\usepackage{algorithm}
\usepackage[noend]{algorithmic}
\usepackage{url}
\usepackage{bm}
\usepackage{graphicx,subfigure}
\usepackage{color}
\usepackage{hyperref}
%\hypersetup{bookmarksopen,
%bookmarksnumbered,
%pdfpagemode=UseOutlines,
%colorlinks=true,
%linkcolor=blue,
%anchorcolor=blue,
%citecolor=blue,
%filecolor=blue,
%menucolor=blue,
%urlcolor=blue
%}


\title{\LARGE \bf
Informed sampling by Markov Chain Monte Carlo methods
}


\author{
Rohan Thakker$^{*}$,
Cole Gulino$^{*}$,
Daqing YI$^{}$,
Oren Salzman$^{}$ and
Siddhartha Srinivasa$^{}$% <-this % stops a space
\thanks{*R. Thakker and C. Gulino contributed equally to this paper.}
\thanks{This work was (partially) funded by the National Science Foundation IIS (\#1409003), Toyota Motor Engineering \& Manufacturing (TEMA), and the Office of Naval Research.}% <-this % stops a space
\thanks{$^{1}${\tt\small \{rthakker, cgulino, daqingy, osalzman, ss5\} @andrew.cmu.edu}}%
%
\\        
Robotics Institute, Carnegie Mellon University$^{1}$
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  macros 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{macros.tex}

\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}

Sampling-based motion-planning algorithms~\cite{CBHKKLT05, L06} have proven to be an effective tool at solving motion-planning problems.
They search through the continuous state space~$\calX$ by sampling random states and maintaining a discrete graph~$G$ called a \emph{roadmap}.
Vertices and edges in $G$ correspond to collision-free states and paths, respectively.

Roughly speaking, all these algorithms iteratively sample new states.
This is required to ensure that, as the number of samples tends to infinity, 
(i)~a solution will be found 
(a property called \emph{probabilistic completeness})
and that
(ii)~given some optimization criteria, the quality of the solution will progressively converge to the quality of the optimal solution
(a property called \emph{asymptotic  optimality}).

In the initial stages of the algorithm, 
when no path has yet to be found, 
the samples need to be drawn from the entire state space~$\calX$.
However, given a path $\gamma$ , the algorithm can limit its sampling domain to a subset of~$\calX$  containing states that \emph{may} produce higher-quality paths than $\gamma$.
Following Gammell et al.~\cite{GSB14}, we call this subset the \emph{informed subset} and denote it $\Cinf$.

For systems without differential constraints, \Cinf can be sampled directly~\cite{GSB14}.
However, for systems with differential constraints, it is not clear how to directly sample \Cinf.
One approach to produce samples in \Cinf is via \emph{rejection sampling}---sampling a state $x \in \calX$ and testing if $x \in \Cinf$.
However, when the size of the informed space~$\Cinf$ is much smaller than entire state space~$\calX$, then this procedure may dominate the running time of the algorithm~\cite{KTC16}.
\emph{Hierarchical rejection sampling} (HRS) is a recently introduced sampling approach that improves the efficiency of rejection sampling.
This is done by sampling in individual dimensions of the state space and progressively combing these partial samples into larger ones while testing if these combined samples may be in \Cinf or not.



While, original sampling-based algorithms such as RRT~\cite{LK01} and PRM~\cite{KSLO96} only guaranteed to asymptotically return \emph{a} solution, if one exists, they did not provide any guarantees on the \emph{quality} of the solution, given some optimization criteria.
Karaman and Frazzoli~\cite{KF11}, presented variants of PRM and RRT, named PRM* and RRT*, respectively that were shown to produce paths who's cost converges asymptotically to the minimal-cost path.
Additional algorithms followed, increasing the converges rate by various techniques such as lazy dynamic programming~\cite{GSB15, JSCP15}, relaxing optimality to near-optimality~\cite{DB14, SH16} and more.


In the most basic form, the motion-planning problem calls for computing \emph{a} path between two given states.
Indeed, 
However, recent years 
Original planners such as RRT~\cite{LK01} and PRM~\cite{KSLO96} did not provide any guarantee on the \emph{s}


\section{Related work}
\label{sec:related_work}


\subsection{Planning}


Classic:\\
RRT~\cite{LK01}

PRM~\cite{KSLO96}

AO:\\
RRT*, PRM*~\cite{KF11}

BIT*~\cite{GSB15}

FMT*~\cite{JSCP15}

Near AO:\\
LBT-RRT~\cite{SH16}

SPARSE~\cite{DB14}


RGGS~\cite{SSH16}


Kinodynamic works:\\~\cite{SL14, XBPA15, WB13, KF10}

\subsection{Sampling}

Hierarchical sampling~\cite{KTC16}.

Informed sampling~\cite{GSB14}

OBPRM~\cite{ABDJV98, YTEA12}

Medial axis~\cite{LTA03, YDLTA14}

heuristic~\cite{US03, SWT09}

\subsection{MCMC}

Markov Chain Monte Carlo(MCMC)~\cite{ADDJ03} is a framework of sampling from a given probability distribution by constructing a Markov chain of random transition toward a desired distribution $ \pi (x) $ .
A set of samples are generated to approximate the desired distribution $ \pi (x) $.
Unlike a rejection sampling, MCMC does a random walk in a parametric space and concentrates on an important areas, which usually brings better efficiency.
Hamiltonian Monte Carlo(HMC)~\cite{N11} is introduced to provide better efficiency in converging to a desired distribution.
It adopts physical system dynamics to propose future states in the Markov chain.
The concept origins from Hamiltonian dynamics, which models the movement of an object.
Given the position $ x $ and the momentum $ q $ of an object, the system energy can be written as $ H(x,q) = U(x) + K(q) $, in which $ U(x) $ is the potential energy and $ K(q) $ is the kinetic energy.
The movement of the object (how $ x $ and $ q $ change over time) is determined by
$ \frac{dx}{dt} = \frac{\partial H}{\partial q} $ and $ \frac{dq}{dt} = - \frac{\partial H}{\partial x} $.
By the association between energy and a canonical distribution, a joint distribution for the states $ x $ and $ q $ can be created from a Hamiltonian dynamics, which is 
$ P(x, q) = \frac{1}{Z} \exp ( -H(x,q) / T ) = \frac{1}{Z} \exp ( -U(x) / T) \exp (-K(q) / T) $.
If we define $ U(x) = - \log ( \pi(x) T ) $, $ P(x, q) $ can be rewritten as $ P(x, q) = \pi (x) \frac{1}{Z} \exp (-K(q) / T) $.
Because the state $ x $ and the the momentum  $ q $ are independent, we can sample $ [ x ,q ] $ from $ P(x, q) $ and discard the momentum $ q $ (as an auxiliary variable).
The collected set of $ x $ is equivalent to sampling from $ \pi(x) $.
We are going to integrate this idea into our sampling framework of efficiently generating samples under kinetic constraints. 

\section{Algorithm}
\label{sec:algorithm}

Define the dynamics of a robot as 
\begin{equation}
\label{eq:kinematics}
\dot{\bm{x}}(t) = f( \bm{x}(t) , \bm{u}(t) ), 
\end{equation}
in which $ \bm{x} \in \mathcal{X} \subseteq \mathbb{R}^n $ is the state of the robot, and $ \bm{u} \in \mathcal{U} \subseteq \mathbb{R}^m $ is the control input.
The dynamics determines a state trajectory $ \pi $, given a duration $ T $, an initial state $ \bm{x}(0) $ and an initial input $ \bm{u}(0) $.
The cost of a trajectory $ \pi $ is defined as the accumulated cost along the path, which is 
\begin{equation}
\label{eq:path_cost}
c(\pi) = \int_0^{T} c( x(t), u(t) ) dt.
\end{equation}
Consider the obstacles in a workspace, we have $ \mathcal{X}_{free} \subseteq \mathcal{X} $.
Consider the bounds of the control input, we have $ \mathcal{U}_{free} \subseteq \mathcal{U} $.
The dynamics of the robot in Equation~\eqref{eq:kinematics}, free state space $  \mathcal{X}_{free} $ and free control input space $ \mathcal{U}_{free} $ determines a free trajectory space $ \Pi_{free} $, which contains all feasible trajectories $ \pi \in \Pi_{free} $.

We can define an optimal kinodynamic path-planning problem that finds a kinodynamic path minimizing the cost in Definition \ref{defn:optimal_kino_path_planning}.
\begin{defn}{ \textbf{Optimal Kinodynamic Path-Planning} }
\label{defn:optimal_kino_path_planning}
	Find an optimal path $  \pi^* $ that satisfies
	\begin{itemize}
	\item $\forall t \in [0,T] $, $ \bm{x} (t) \in \mathcal{X}_{free}  $ and $ \bm{u} (t) \in \mathcal{U}_{free} $;
	\item $ \bm{x} (0) = x_{init} $, $ \bm{x} (T) = x_{goal} $, $ \bm{u} (0) = u_{init} $ and $ \bm{u} (T) = u_{goal} $; and
	\item $ \pi^* = \max_{ \pi \in \Pi_{free} } c( \pi ) $. 
	\end{itemize}
\end{defn}

\dy{Do we want to set an initial and a goal for the control input? Does this simplify the problem solving?}

\begin{algorithm}
	\begin{algorithmic}[1]
		\STATE
   	\end{algorithmic}
	\caption{MCMC Informed Sampling}
	\label{alg:mcmc_informed_sampling}
\end{algorithm}



\section{Results}


\section{Conclusion}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliography{bibliography}
\bibliographystyle{IEEEtran}
%\bibliography{IEEEabrv,bibliography}


\end{document}
