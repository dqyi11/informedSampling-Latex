%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

\makeatletter
\let\NAT@parse\undefined
\makeatother

\usepackage{xspace}
\usepackage{amsmath,amssymb,amsfonts,amsfonts}
\usepackage{algorithm}
\usepackage[noend]{algorithmic}
\usepackage{url}
\usepackage{bm}
\usepackage{graphicx,subfigure}
\usepackage{color}
\usepackage{cite}
\usepackage{hyperref}
%\hypersetup{bookmarksopen,
%bookmarksnumbered,
%pdfpagemode=UseOutlines,
%colorlinks=true,
%linkcolor=blue,
%anchorcolor=blue,
%citecolor=blue,
%filecolor=blue,
%menucolor=blue,
%urlcolor=blue
%}


\title{\LARGE \bf
Informed sampling by Markov Chain Monte Carlo methods
}


\author{
Rohan Thakker$^{*}$,
Cole Gulino$^{*}$,
Daqing Yi$^{}$,
Oren Salzman$^{}$ and
Siddhartha Srinivasa$^{}$% <-this % stops a space
\thanks{*R. Thakker and C. Gulino contributed equally to this paper.}
\thanks{This work was (partially) funded by the National Science Foundation IIS (\#1409003), Toyota Motor Engineering \& Manufacturing (TEMA), and the Office of Naval Research.}% <-this % stops a space
\thanks{$^{1}${\tt\small \{rthakker, cgulino, daqingy, osalzman, ss5\} @andrew.cmu.edu}}%
%
\\        
Robotics Institute, Carnegie Mellon University$^{1}$
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  macros 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{macros.tex}

\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}


Sampling-based motion-planning algorithms~\cite{CBHKKLT05, L06} have proven to be an effective tool at solving motion-planning problems.
They search through a continuous state space~$\calX$ by sampling random states and maintaining a discrete graph~$G$ called a \emph{roadmap}.
Vertices and edges in $G$ correspond to collision-free states and paths, respectively.

Roughly speaking, these algorithms iteratively sample new states.
This is required to ensure that, as the number of samples tends to infinity, 
(i)~a solution will be found 
%(a property called \emph{probabilistic completeness})
and that
(ii)~given some optimization criteria, the quality of the solution will progressively converge to the quality of the optimal solution.
%(a property called \emph{asymptotic  optimality}).

Initially, 
when a path has yet to be found, 
the samples are drawn from the entire state space~$\calX$.
However, once a path $\gamma$ is produced,  algorithms that seek \emph{high-quality paths} can limit their sampling domain to a subset of~$\calX$ only  containing states that may be used to produce higher-quality paths than~$\gamma$.
Following Gammell et al.~\cite{GSB14}, we call this subset the \emph{informed subset} and denote it~$\Cinf$.
\textbf{In this work we address the problem of efficiently producing samples in informed subset for systems with arbitrary complex costs}. 

For Euclidean spaces optimizing for path length, 
\Cinf can be analytically expressed as a prolate hyperspheroid and can be sampled directly using a closed-form solution~\cite{GSB14}.
Indeed, directly sampling in \Cinf has been shown to dramatically improve computation time when compared to sampling in~$\calX$, especially in high dimensions. 

Unfortunately, in more general settings,
it is not clear how to directly sample \Cinf.
%
One approach to produce samples in \Cinf is via \emph{rejection sampling}---sampling a state $x \in \calX$ and testing if $x \in \Cinf$.
However, when the size of the informed space~$\Cinf$ is much smaller than entire state space~$\calX$, this procedure is highly inefficient, dominating the running time of the algorithm~\cite{KTC16}.
Recently, Kunz et al.~\cite{KTC16} showed, under some technical assumptions, how to partially ameliorate this inefficiently by \emph{Hierarchical rejection sampling} (HRS). 
Here, individual dimensions are sampled recursively 
and then combined. Rejection sampling is performed for these partial samples until a suitable sample has been produced. 
%The novelty of HRS is the ability to fail fast. 
Unfortunately, HRS may still produce a large number of rejected samples especially in high-dimensional spaces~\cite{KTC16}.
This may cause the planning algorithm to spend most of its time trying to produces samples in~$\Cinf$ rather than explore it.

In this paper, we suggest an alternative approach to produce samples in the informed set \Cinf for a wide range of settings.
\textbf{
Our main insight is to recast this problem as one of sampling uniformly within a level set of an implicit function.
This recasting enables us to apply Monte Carlo sampling methods, used very effectively in the optimization community, to solve our problem.
}
%
%\textbf{Our main insight is to formulate the problem as the problem of sampling uniformly below a level set of an implicit function and then apply optimization and statistical tools.}
\os{do we really care about uniformity?}
\dy{Is it fair to say that uniformity guarantee each state has equal opportunity of being selected?}
Specifically, our approach consists of two stages:
in the first, a random sample $x \in \calX$ is retracted to the boundary of~$\Cinf$ by running a root-finding algorithm;
in the second stage, this retracted sample  is used to seed a Monte Carlo sampling chain which allows us to  produce samples that (approximately) cover~$\Cinf$  uniformly.

Our approach requires that the system has a solution to the two-point boundary value problem (2pBVP)~\cite{L06, H02} and that a gradient can be defined over the cost function.
Indeed, we demonstrate the efficiency of our approach on a wide variety of systems and show that it enables reducing the planning time by several orders of magnitute when compared to algorithms using rejection sampling or HRS.

\os{structure}

%While, original sampling-based algorithms such as RRT~\cite{LK01} and PRM~\cite{KSLO96} only guaranteed to asymptotically return \emph{a} solution, if one exists, they did not provide any guarantees on the \emph{quality} of the solution, given some optimization criteria.
%Karaman and Frazzoli~\cite{KF11}, presented variants of PRM and RRT, named PRM* and RRT*, respectively that were shown to produce paths who's cost converges asymptotically to the minimal-cost path.
%Additional algorithms followed, increasing the converges rate by various techniques such as lazy dynamic programming~\cite{GSB15, JSCP15}, relaxing optimality to near-optimality~\cite{DB14, SH16} and more.
%
%However, recent years 
%Original planners such as RRT~\cite{LK01} and PRM~\cite{KSLO96} did not provide any guarantee on the \emph{s}


\section{Related work}
\label{sec:related_work}
We start in Sec.~\ref{subsec:planning} by giving an overview of relevant sampling-based motion-planning algorithms.
We then continue in Sec.~\ref{subsec:sampling} to describe different approaches that can be used by  these algorithms to sample~$\calX$.
We conclude our literature review in Sec.~\ref{subsec:mcmc} with a brief overview of Markov Chain Monte Carlo methods.

\subsection{Sampling-based motion-planning algorithms}
\label{subsec:planning}
Initial sampling-based algorithms such as RRT~\cite{LK01} and PRM~\cite{KSLO96} did not take into account the \emph{quality} of a path, given some optimization criteria, and only guaranteed to asymptotically return \emph{a} solution, if one exists.
Karaman and Frazzoli~\cite{KF11}, presented variants of PRM and RRT, named PRM* and RRT*, respectively that were shown to produce paths who's cost converges asymptotically to the minimal-cost path.
This was done by recognizing the underlying connections between stochastic sampling-based motion planning and the theory of random geometric graphs (see also~\cite{SSH16}).
Additional algorithms followed, increasing the converges rate by various techniques such as 
lazy dynamic programming~\cite{GSB15, JSCP15, SH15},
relaxing optimality to near-optimality~\cite{DB14, SH16} 
and more.


Many of the algorithms mentioned require solving a two-point boundary value problem (2pBVP) to perform exact and optimal connections between vertices in the roadmap.
For holonomic robots, these are simply straight lines in the configuration space, but for kinodynamic sytems with arbitrary cost functions,  computing an optimal trajectory between two states is non-trivial in general.

Xie et al.~\cite{XBPA15} use a variant of sequential quadratic programming (SQP) to solve 2pBVP and integrate it with BIT*~\cite{GSB15}.
Webb and van den Berg~\cite{WB13} use a fixed-final-state-free-final-time controller to solve the 2pBVP  with respect to a cost function that allows for balancing between the duration of the trajectory and the expended control effort.
Perez et al.~\cite{PPKKL12} propose a variant of RRT* that automatically defines a distance metric and node extension method by locally linearizing
the domain dynamics and applying linear quadratic regulation (LQR).

Finally, we note that we are not the first to integrate Monte Carlo sampling into planning algorithms. T-RRT~\cite{JCS10} and its variants~\cite{DSC13} are inspired by Monte Carlo optimization techniques and use notions such as the Metropolis criterion~\cite{CG95} to guide the exploration of the configuration space.

%Finally, we note that there have been many papers presenting solutions to the 2pBVP for specific systems such as
%
%Jeong Hwan Jeon, Raghvendra V. Cowlagi, Steven C. Peters, Sertac Karaman, Emilio Frazzoli, Panagiotis Tsiotras, Karl Iagnemma: Optimal motion planning with the half-car dynamical model for autonomous high-speed driving. ACC 2013: 188-193
%Dubins cars optimizng for both time and energy (RSS submission 107 by Shammas, Elie) 

%possible additional papers on Kinodynamic RRT*:~\cite{KF10, GPPK13}

\subsection{State-space sampling}
\label{subsec:sampling}
There is a rich body of literature on how to produce samples that increase the efficiency of a planner in terms of finding a solution or producing high-quality solutions.
Heuristic approaches include
sampling on the medial axis~\cite{WAS99a, WAS99b, LTA03, YDLTA14},
sampling near the boundary of the obstacles~\cite{ABDJV98, YTEA12},
resampling along a given trajectory~\cite{APD11, AS11}
and more~\cite{US03, SWT09}.
For planning under the differential constraints,
reachability-guided sampling~\cite{SWT09, PLAEFRA17} focuses on sampling regions of the state space that are most likely to promote expansion for the given constraints.


Of specific interest to our work are approaches that produce samples in the informed set~\Cinf.
As mentioned in Sec.~\ref{sec:intro} Gammel et al.~\cite{GSB14} describe an approach to sample uniformly in~\Cinf for the specific case of where $\calX = \R^d$ and when optimizing for path length.
To the best of our knowledge, the only method to produce samples in non-Euclidean spaces (other than rejection sampling) is HRS by Kunz et al.~\cite{KTC16}.

\subsection{Markov Chain Monte Carlo (MCMC)}
\label{subsec:mcmc}
\os{This section puts a lot of emphasis on HMC. If we end up choosing an alternative method, we should change the relevant weight}
Monte Carlo simulation is a general sampling framework widely used in various domains such as
statistical machine learning~\cite{M97},
motion tracking~\cite{KBD04}, 
data regression~\cite{TL11} and 
state estimation~\cite{ASC13}.
Roughly speaking, Monte Carlo simulation repeatedly samples a domain at random to approximate some value or function.
One specific domain where Monte Carlo simulation is used which is relevant to this work is generating draws from a desired distribution which is hard to sample directly.

One of the popular classes of Monte Carlo simulation is 
\emph{Markov Chain Monte Carlo} (MCMC)~\cite{ADDJ03}.
Here, the samples are drawn by generating a Markov chain such that the distribution of points on the chain converges to the desired distribution.
The Markov Chain can be generated by performing a random walk such as
in Metropolis-Hastings~\cite{CG95}, 
Gibbs sampling~\cite{CK94},
hit-and-run~\cite{S84,KSZ11} and their many variants.
Alternatively, samples on the chain can be generated by simulating Hamiltonian dynamics~\cite{N94} (instead of performing the random walk).
This approach, termed Hybrid or Hamiltonian Monte Carlo (HMC)~\cite{N11} was shown to converge faster to the desired distribution, when compared to random-walk based MCMC algorithms.

%\deprecated{
\ignore{ which means that future states in the Markov chain is proposed from simulating system dynamics instead of sampling from a jumping distribution.
%
%
An alternative sampling framework, which is widely used in statistical machine learning, is Monte Carlo Simulation~\cite{M97}.
%
It uses the numerical experimental power of computers to generate samples toward a desired distribution $ \pi (x) $.
The generated set of samples are a discrete approximation $ \hat{\pi} (x) $ of the desired distribution $ \pi (x) $.
A approximated desired distribution $ \hat{\pi} (x) $ can be used in any problem that states are modeled random variable, e.g. motion tracking~\cite{KBD04}, data regression~\cite{TL11} and state estimation~\cite{ASC13}.
%
Markov Chain Monte Carlo (MCMC)~\cite{ADDJ03} is one most popular class that constructs a Markov chain of random transition that moves samples from an original proposed distribution to a desired distribution $ \pi (x) $.
The random movement is driven by a jumping distribution $ g(x' \mid x) $, which samples a new state $ x' $ by a given state $ x $.
The idea of MCMC is designing a jumping distribution $ g $ that makes the desired distribution a stationary distribution $ \pi = \pi * g $.
\emph{Ergodic theorem} shows that if the jumping distribution is aperiodic, irreducible and positive recurrent~\cite{AG11}, samples generated from arbitrary distributions will gradually converge to the stationary (desired) distribution $ \pi $.
In practice, samples are initially generated from an arbitrary distribution.
They shall converge to the samples generated from the desired distribution () 
A few MCMC algorithms, e.g. Gibbs sampling and Metropolis-Hastings, have been widely used in sampling from distributions that are difficult to directly sample.
%
The nature of random walk in MCMC sometimes requires too many iterations to converge, especially in high-dimension state spaces.
}

\section{Problem definition}
\label{sec:algorithm}
Let $\calX, \calU$ denote the state and controls spaces, respectively and set $\Cfree \subset \calX$ to be the set of states where the robot is collision free.
A \emph{trajectory} $\gamma$ is a timed path through~$\calX$ obtained by applying at time $t$ control $u(t) \in \calU$ and satisfying the system dynamics 
$\dot{x}(t) = f( x(t) , u(t) )$.
A trajectory is collision free if $\forall t,~\gamma(t) \in \Cfree$

Given a cost function $C : \calX \times \calU \rightarrow \R$, the cost of a trajectory $ \gamma $ is the accumulated cost along the path
$c(\gamma) = \int_0^{T} c( x(t), u(t) ) |\dot{\gamma}(t)|dt$, 
where $T$ is the duration of~$\gamma$.

Given start and target states $x_s, x_g \in \calX$, we wish to find a collision free trajectory $\gamma^*$ connecting $x_s$ to $x_g$ such that 
$c(\gamma^*) = \min_{\gamma \in \Gamma} c(\gamma)$, where $\Gamma$ is the set of all collision-free trajectories.

Given a trajectory $\gamma_{\text{best}}$ with cost $c_{\text{best}} = c(\gamma_{\text{best}})$ the \emph{informed set}~\Cinf is defined to be all states $x$  which may be on trajectories with lower cost than $c_{\text{best}}$.
Specifically,
$
\Cinf = \{ x \in \calX \mid  
		c ( \gamma^*(x) ) < \cbest \} $~\cite{GSB14}.
Here~$ \gamma^*(x) $ denotes the optimal trajectory  from $ x_s $ to $ x_g $ constrained to pass through $ x $.
Notice that we do not require that~$ \gamma^*(x) $ is collision free.
\os{add fig}



\ignore{
Define the dynamics of a robot as 
\begin{equation}
\label{eq:kinematics}
\dot{x}(t) = f( x(t) , u(t) ), 
\end{equation}
in which $ x \in \calX \subseteq \mathbb{R}^n $ is the state of the robot, and $ u \in \calU \subseteq \mathbb{R}^m $ is the control input.
The dynamics determines how a state trajectory $ \gamma $ is like given duration $ T $, state $ x(t) $ and input $ u(t) $.
There exists a free trajectory space $ \Pi_{free} $ that contains all feasible trajectories $ \gamma \in \Gamma_{\rm free} $.
It is determined by the system dynamics, $\Cfree $ and $ \Ufree $, in which $ \Cfree \subseteq \calX $ is a free state space considering the obstacles in a workspace, and $ \Ufree \subseteq \calU $ is a free control space considering the bounds of control input. 
The cost of a trajectory $ \gamma $ is defined as the accumulated cost $ c( x(t), u(t) ) $ along the path, which is 
\begin{equation}
\label{eq:path_cost}
c(\gamma) = \int_0^{T} c( x(t), u(t) ) dt.
\end{equation}
An optimal kinodynamic path-planning problem that finds a kinodynamic path minimizing the cost $ \pi^* = \min_{ \pi \in \Gamma_{free} } c( \gamma ) $.

Informed sampling-based planning algorithms (e.g. Informed RRT*~\cite{GSB14}, BIT*~\cite{GSB15}) samples new states only from an \emph{informed set}.
An informed set is a subspace $ \Cinf \subseteq \calX $   that is possible to provide better solution than the current best $  \Cinf = \{ x \in \calX \mid c ( \gamma^*(x) ) < \cbest \} $~\cite{GSB14}, in which $ \gamma^*(x) $ means the optimal path from $ x_s $ to $ x_g $ constrained to pass through $ x $ without considering collision checking, and $ \cbest $ is the cost of the current best found.
The current best $ \cbest $ constrains eligible $ x $ indirectly by an implicit $ \gamma^*(x) $.
In an optimal kinodynamic path-planning problem, more constraints are considered in defining an informed set, which include the system dynamic constraint and the boundary of control input.
If we consider a manifold that is defined by an informed set $ \Cinf $, the informed sampling becomes how to sample on this manifold in a high-dimensional space. }

\section{Motivating example---Minimal-Time Double Integrator (MTDI)}
\label{sec:mtdi}

To understand why we resort to optimization-based methods and do not attempt to provide a closed-form solution to sample $\Cinf$ we study the structure of the informed set for a simple yet important dynamical system---the double integrator minimizing time (MTDI). 
Here, we are given a one-dimensional point robot with bounded acceleration moving amid obstacles. We wish to compute the minimal-time trajectory between two states $x_s, x_g$.
A state $x \in \calX$ in this model is defined by 
the position $q \in \R$
and
the velocity $\dot{q}\in \R$ of the robot.
%each coordinate axis.
The system dynamics are described by:
\begin{equation}
\begin{bmatrix}
	\dot{q} \\
	\ddot{q}
\end{bmatrix}
=
\begin{bmatrix}
	0 & 1 \\
	0 & 0
\end{bmatrix}
\begin{bmatrix}
	{q} \\
	\dot{q}
\end{bmatrix}
+
\begin{bmatrix}
	0 \\
	1
\end{bmatrix}
u
\end{equation}
%\begin{equation}
%x = (q, \dot{q}); 
%\hspace{5mm}
%u = \ddot{q}.
%\end{equation}
Here, the control 
%$u \in [u_{\text{min}}, u_{\text{max}}]$ 
$u \in [\underline{u}, \overline{u}]$ 
is the (bounded) acceleration\footnote{One can also bound the velocity of the system. However, to simplify the exposition, we only constrain the system with acceleration bounds.}. 


Notice that 
(i)~this is model can be seen as a simplified one-dimensional instance of a robot manipulator with many degrees of freedom and that
(ii)~closed-form solutions exist to the 2pBVP for this specific case (as well as the multi-dimensional setting)~\cite{HN10, KS14}.

%Let $T(x_1, x_2)$ denote the minimal time to move the robot from state $x_1$ to $x_2$.
%Furthermore, set
%%
%\begin{equation}
%T(x_s, x, x_g) = T(x_s, x) + T(x, x_g).
%\end{equation}
%%
%Namely, $T(x_s, x, x_g)$ is the minimal time to reach the goal from the start subject to passing through the state $x$.
%
%
%\subsection{Informed set}
Recall that for Euclidean spaces minimizing path length, the informed set~\Cinf is a prolate hyperspheroid.
Moreover, the size and shape of the hyperspheroid is defined only be the cost $c_{\text{best}}$ of the current best solution and not by the location of the start~$x_s$ and goal~$x_g$.

For the case of a MTDI, this is not the case. 
Specifically, we have that 
(i)~the structure of~\Cinf changes not only with~$c_{\text{best}}$ but also according to the specific values of~$x_s$ and~$x_g$ 
and that
(ii)~the cost map that implicitly defines~\Cinf can contain discontinuities (in contrast to Euclidean spaces minimizing path length where the cost map is continuous and differentiable at every point).

To understand the differences recall that optimal trajectories  for MTDI follow a ``bang-bang`` controller~\cite{HN10, KS14}.
Namely, we first apply maximal (or minimal) acceleration for some duration and then switch to applying minimal (or maximal, respectively) acceleration.
It is straightforward to see that both the type and the amount of acceleration applied (and hence the structure of~\Cinf) depend on the specific values of~$x_s$ and~$x_g$. 
Fig~\ref{fig:informed_sets} 
depicts two informed sets defined using the same cost~$c_{\text{best}}$ but different start and goal states. Notice the difference in structure of~\Cinf. 
Fig~\ref{fig:discont} depicts a simple example where the cost map is discontinuous.

To summarize, the structure of \Cinf can change given different start and goal states.
Furthermore,  its boundary may not be  differentiable due to the aforementioned discontinuous. 

\ignore{
See Fig. ~\ref{fig:sample_dimt} for an example motion of one joint through the $\left(q,\dot{q}\right)$ space. 

\begin{figure}[tb]
  \centering
  	\includegraphics[width=0.3\textwidth]{fig/example_motion.png}
  \caption{
    \captionstyle
  	Example possible motion using the double-integrator minimum time model between start state $(x_1, \dot{x_1})$ and goal state $(x_2, \dot{x_2})$ through some intermediate state $(x_i, \dot{x_i})$.
  	}
   	\label{fig:sample_dimt}
\end{figure}

In order to find the minimum time, $T\left(x\right)$, of a motion like Fig. ~\ref{fig:sample_dimt}, we can examine the paths separately with one corresponding to a motion with acceleration $a_1$ from $\left(x_1, \dot{x}_1\right)$ to $\left(x_i, \dot{x}_i\right)$ and another corresponding to motion with acceleration $a_2$ from $\left(x_i, \dot{x}_i\right)$ to $\left(x_2, \dot{x}_2\right)$.

\ref{} shows that we can express total distance travelled in the joint space $x$ can be expressed in this way as:

\begin{equation}
x_2 - x_1 = d_{a_1} + d_{a_2}
\end{equation}
where $d_{a_1}$ is the distance travelled at max acceleration from the first path and $d_{a_2}$ is the distance travelled at max acceleration in the opposite direction.

Using simple constant acceleration equations we can express this in the form:

\begin{equation}
x_2 - x_1 = \left(\dot{x}_1 t_{a_1} + \frac{1}{2} a_1 t_{a_1}^2\right) + \left(\frac{\dot{x}_2^2 - \dot{x}_i^2}{2a_2}\right)
\end{equation}
where $t_{a_1}$ is the time it takes to make the path from $\left(x_1, \dot{x}_1\right)$ to $\left(x_i, \dot{x}_i\right)$ at acceleration $a_1$ and $t_{a_2}$ is the time it takes to make the path from $\left(x_i, \dot{x_i}\right)$ to $\left(x_2, \dot{x}_2\right)$ at acceleration $a_2$.

We can further expand the equation in order to put it into the canonical quadratic equation as follows:

\begin{equation}
a_1 t_{a_1}^2 + 2 \dot{x}_1 t_{a_1} + \frac{\dot{x}_2^2 - \dot{x}_1^2}{2a_2} - \left(x_2 - x_1\right) = 0
\end{equation}

Following ~\ref{} we solve a quadratic equation of the form $at^2+bt+c=0$ as follows:

$$
q = -\frac{1}{2}\left(b + \text{sign}\left(b\right)\sqrt{b^2 - 4ac}\right)
$$
$$
t_1 = \frac{q}{a}; \hspace{5mm} t_2 = \frac{c}{q}
$$
where $t_1$ and $t_2$ are the two solutions to the quadratic equation.

As we know, time must be positive and we are looking for minimum time. This leads to the fact that there is only one correct solution to the quadratic equation. Because of this, depending on the start and goal states, the sign of the acceleration on each path will change.

Again following ~\ref{}, we programatically solve for the sign of the sign of the acceleration by analysing the start and goal position. Fig. ~\ref{fig:sample_dimt} shows the determining term $\Delta x_{acc}$, which is the distance from $x_1$ to the point where $\dot{x} = \dot{x}_2$. 

And using constant acceleration equations with ~\ref{} again, we find that:

\begin{equation}
\Delta x_{acc} = \frac{1}{2} \left(\dot{x}_1 + \dot{x}_2\right) \frac{|\dot{x}_2 - \dot{x}_1|}{a_{max}}
\end{equation}

\begin{equation}
a_1 = - a_2 = \text{sign}\left(x_2 - x_2 - \Delta x_{acc}\right) a_{max}
\end{equation}

From these equations it shows a fundamental difference of a kinodynamic state space compared to a geometric state space. Because of this the minimum time path can vary in shape depending on the start and goal state. The consequence of this is that the shape of the informed subset can change dramatically depending on the start and goal state of the problem definition. Because of this, finding a closed form solution to sampling in the informed space is not feasible.
}

\ignore{
See Fig.~\ref{fig:informed_1d_di} for a visualization of the cost surface for a specific start and goal state for one degree of freedom (two dimensions $\left(x, \dot{x}\right)$). Lower costs are represented with cooler colors with warmer colors representing higher costs. One can see the minimum cost path from start to goal as the dark blue curve.
}

\begin{figure}[tb]
  \centering
  	\includegraphics[height = 5.25cm ]{fig/cost_discontinuity.pdf}
  \caption{
    \captionstyle
  	Visualization of the discontinuity in the cost function of MTDI (right) related to the types of controls applied (left). 
  	Given state~$x_s$ and fixed position $q_0$, we depict the cost (time) as a function of the velocity~$\dot{q}$. 
  	The minimal cost is attained at $\dot{q}_{\min}$ by applying maximal acceleration (blue curves~$(i), (ii)$). 
  	To reach states such as~$\dot{q}_1$, where $\dot{q}_1 < \dot{q}_{\min}$ we need to apply maximal acceleration (curve~$(i)$) followed by minimal acceleration (green curve~$(iii)$), which result in a continuous increase in cost.
  	However, for states such as $\dot{q}_2$, where $\dot{q}_2 > \dot{q}_{\min}$, we need to apply minimal acceleration  followed by maximal acceleration (curves~$(iv), (v)$), which result in the discontinuity.
  	}
   	\label{fig:discont}
\end{figure}

\ignore{
\begin{figure}[tb]
  \centering
  	\includegraphics[height = 4.cm ]{fig/level_set.jpg}
  \caption{
    \captionstyle
  	Level sets for the case of minimal-time double integrator. A point $x = (q, \dot{q})$  denote the position and velocity of the robot while the color at $x$ denotes the cost to reach $x_t$ from $x_s$ subject to passing through the state $x$.
  	}
   	\label{fig:informed_1d_di}
	\vspace{-5.5mm}
\end{figure}

\begin{figure}[tb]
  \centering
  	\includegraphics[width=0.5\textwidth]{fig/kino_space.png}
  \caption{
    \captionstyle
  	Side view of the cost surface for the case of minimal-time double integrator. The side view showcases the discontinuous nature of the cost surface.
  	}
   	\label{fig:cost_surface_1d}
%	\vspace{-5.5mm}
\end{figure}

See Fig.~\ref{fig:cost_surface_1d} for a side view of the cost surface for one degree of freedom ($\left(x, \dot{x}\right)$). One can see a key property of the cost surface that cause serious concerns is the discontinuous nature of the surface. As mentioned before, finding the minimum time problem amounts to solving a quadratic equation. Because the nature of this equation changes with the start and end goal, there is a curve in the input space that amounts to a switching surface that represents the boundary for the decision of the sign of $a_1$. This surface causes a discontinuous cost surface as shown in Fig. ~\ref{fig:cost_surface_1d}.
}

\section{MCMC-based Informed Sampling}
In this section we describe our approach to efficiently produce samples in the informed set~\Cinf given a specific cost~$c_{\text{best}}$.
Our algorithm (see Alg.~\ref{alg:mcmc_informed_sampling}) starts by randomly sampling a state~$x_\text{rand} \in \calX$ (line~1).
This sample is used to produces a 
sample~$x_0 \in \partial\Cinf$ 
%sample~$x_\text{boundary} \in \partial\Cinf$ 
which lies (approximately) on the boundary of the informed set (Sec.~\ref{subsec:grad})by applying Newton-Raphson method~\cite{RT06} (line~2).
Finally,~$x_0$ 
%use~$x_\text{boundary}$ 
is used (lines~3-5)
to initialize an MCMC algorithm which produces a series of samples within \Cinf (Sec.~\ref{subsec:mcmc}). Specifically, we chose to use HMC due to its favourable properties (see Sec.~\ref{sec:intro}). 
For a visualization of the approach, see Fig.~\ref{fig:alg}.

We apply MCMC by using the cost function to define a probability distribution \Pinf.
Specifically, points in \Cinf are distributed uniformly
while 
the probability of sampling any configuration $x \in \calX \setminus \Cinf$ is zero.
The MCMC algorithm will allow us to  efficiently produce samples (approximately) from \Pinf.

\begin{figure}[tb]
  \centering
  	\includegraphics[trim={4.5cm 0 4cm 2cm},clip,height = 5.25cm ]{fig/alg.pdf}
  \caption{
    \captionstyle
  	Algorithmic approach.
  	Cost function is depicted using isocontours (darker shades reflect lower cost) while the boundary of the informed set is depicted in purple. 
  	The root-finding and MCMC algorithms are depicted in blue and green, respectively.
  	}
   	\label{fig:alg}
\end{figure}

\begin{algorithm}[t]
	\begin{algorithmic}[1]
		\STATE $x_{\text{rand}} \leftarrow \texttt{sample\_uniform(\calX)}$
%		\STATE $x_{\text{boundary}} 
		\STATE $x_{0} 
		 \leftarrow
		 \texttt{newton\_raphson}(x_{\text{rand}}, c_{\text{best}})$
%		\STATE $x_{0} \leftarrow x_{\text{boundary}}$
		\FOR {$i = 1$ to $N$}
			\STATE $x_{i} \leftarrow \texttt{MCMC} (x_{i-1}, c_{\text{best}})$
			\STATE \textbf{output} $x_{i}$
		\ENDFOR
		 
   	\end{algorithmic}
	\caption{MCMC Informed Sampling $(c_{\text{best}})$}
	\label{alg:mcmc_informed_sampling}	
\end{algorithm}

\subsection{Sampling the boundary of \Cinf via Newton-Raphson}
\label{subsec:grad}
In theory, MCMC methods converge to the desired distribution regardless of the initial sample used to seed the chain.
However, in practice it is common to discard an initial set of samples (a process referred to as ``burn-in'') to avoid starting biases~\cite{ADDJ03}. 

In our setting, we are only interested in points in~\Cinf, thus we can start the Markov Chain in~\Cinf and avoid this burn-in stage. 
To do so we apply  Newton-Raphson's method to obtain a sample on $\partial \Cinf$. 
However, our cost function is not necessarily continuous (see Sec.~\ref{sec:mtdi}), thus if no sample was produced on $\partial \Cinf$ after a predefined number of steps, we restart the algorithm using a different random state~$x_\text{rand}$.


\subsection{Sampling the interior of \Cinf via
MCMC}
\label{subsec:mcmc}
\os{once we have results we need to discuss here (or in the implementation details), how is this used: Do we use one chain? Do we restart from scratch? Do we use every sample on the chain?}


	Roughly speaking, HMC views the system as an energy function for which Hamiltonian dynamics are simulated in fixed time steps (for additional background on HMC see Appendix).
	Thus, to sample from a desired distribution~$\pi$ using HMC, we need to convert the distribution to an energy function and vice-versa.
	This is done using the concept of a canonical distribution from statistical mechanics~\cite{N11}. 
	Given some energy function,~$E$, the canonical distribution over states has the Boltzman probability distribution $\pi = \frac{1}{Z}\exp (-E/T)$, where $Z$ is a normalizing constant and~$T$ is the temperature of the system.
	%Viewing this the opposite way, if we are interested in some distribution with density function $P$, we can obtain it as a canonical distribution with $T = 1$ by setting $E = -\log P - \log Z$, where $Z$ is any convenient positive constant	

The energy function's definition provides the flexibility to specify desired or more probable states (by presenting lower energies) without the need to use a formal and restrictive model. 
Specifically, in our setting we want to approximate 
a uniform distribution over points inside the informed set~\Cinf.

Our energy for any given cost $c$, 
denoted by~$E\left(c,\Cbest \right)$, 
is a function of $c$ and the current best cost~$\Cbest$.
To consider only points in the informed set, all with equal energy, one may consider taking 
$$
E\left(t,\Cbest \right) = 
\begin{cases}
    0,	& \text{if } c \leq \Cbest\\        
    \infty,  	& \text{otherwise}.
\end{cases}
$$
However, simulating Hamiltonian dynamics requires that the energy function defined is differentiable.
This function has a discontinuity at $c = \Cbest$ 
and contains no gradient information for $c \leq \Cbest$.
Thus, we define our energy function\footnote{In practice we add another term to the energy function which we discuss in Sec.~\ref{sec:eval}. We omit the term here for clarity of exposition.} as  
\begin{equation}
\label{eq:energy}
E\left( c, \Cbest \right) 
= 
E_{\text{grad}} \left( c \right) + 
E_{\text{informed}} \left( c,\Cbest \right).
\end{equation}
Here, $E_{\text{grad}}$ is a user-defined function that
controls the ``flatness`` of the distribution.
Intuitively, this term biases the distribution towards states with smaller cost and balances between the uniformity of the distribution and the gradient information provided to the HMC algorithm.
We used $E_{\text{grad}} (c) = \tanh(c + \alpha )$,  which whose gradient is close to zero for $c>0$.
Here, $\alpha$ is some constant that controls the slope of the function\footnote{In our implementation we use $\alpha=3$.}.

The term 
$E_{\text{informed}} \left( c,\Cbest \right) 
= \beta \cdot \text{sigm}\left(\Cbest - c\right)
$
penalizes states outside the informed set~$\Cinf$ using a tight sigmoid function~$\text{sigm}$ scaled by some (large\footnote{The constant~$\beta$ gives points out of~\Cinf a large energy value which in turn implies a low probability of being sampled. In theory, we would like to have~$\beta \approx \infty$. However, choosing~$\beta$ to be too large may cause numerical instabilities.}) constant~$\beta$.
The sigmoid can be seen as a differentiable function smoothing the boundary of~$\Cinf$.
See Fig.~\ref{fig:energy} for a visualization.

\begin{figure}[tb]
  \centering
  	\includegraphics[height = 4.5cm ]{fig/energy.pdf}
  \caption{
    \captionstyle
  	Energy function used by HMC to obtain samples~$x \in \Cinf$.
  	}
   	\label{fig:energy}
\end{figure}

\subsection{Implementation details}
In our implementation we add another term to our energy function (Eq.~\ref{eq:energy}) relating to system parameters that induce known bounds on states.
Specifically, our robot has predefined joint limits. 
The flexibility of the energy function allows us to incorporate these limits  to penalize states \emph{within} the informed set that are violating these limits.

Specifically, we add to Eq.~\ref{eq:energy} the term 
$
E_{\text{limits}}\left(\mathbf{x}\right) = \beta\cdot \text{sigm} ( ||\mathbf{x}_{\text{limits}} - \mathbf{x}||_2 )
$
where $||\mathbf{x}_{\text{limits}} - \mathbf{x}||_2$ represents the L2 distance between the current state $\mathbf{x}$ and the state limits~$\mathbf{x}_{\text{limits}}$.
 

\subsection{Probabilistic completeness}

%\newpage
%\begin{thm}
%For each node $u$ that was removed from the priority $\calQ$, $g(u) = \delta(u)$, where $\delta(u)$ denotes the shortest path to reach $u$ from $s$.
%\end{thm}
%
%The proof is by induction on the order of vertices removed from $\calQ$. 
%
%\textbf{Base} holds trivially for $s$.
%
%\textbf{Step:}
%Let $v$ be the current node on the top of the priority queue and let $u$ be its parent.
%Notice that by the induction hypothesis we have that $g(u) = \delta(u)$.
%
%We have two case to consider: 
%(i) the shortest path does not pass through $u$ but through some other node $u'$ which is not in $\calQ$,
%and
%(i) the shortest path does not pass through $u$ but through some other node $y$ which is in $\calQ$.
%
%For case (i), we have that when $u'$ was removed from $\calQ$, then $v$'s cost was $g(v) \leq g(u') + c(u',v)$.
%Since $g(u') = \delta(u')$ this is equal to $\delta(v)$.
%
%For case (ii), let $x$ be $y$'s parent which was already removed from $\calQ$.
%Note that $g(x) = \delta(x)$ and (similar to the previous case) we have that $g(y) < g(x) + c(x,y)$.
%Furthermore notice that $v$ was before $y$ in $\calQ$ thus $g(v) \leq g(y)$.
%Purring everything together we have that
%$$
%\delta(v)   > \delta(x) + c(x,y)
%			\geq g(y)
%			\geq g(v).
%$$
%Here, the first transition is due to the assumption that the shortest path goes from $s$ to $v$ through the edge $(x,y)$.
%The second (which I got confused about in class) is due to the fact that when $x$ was popped from $\calQ$ it updated $y$'s cost.
% \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\
% 
%\vspace{20cm}

\ignore{
\section{MCMC-based Informed Sampling}

Rejection sampling is the simplest ways of constrained sampling~\cite{CRCW04}, but is also known for high rejection-rate on a manifold in a high-dimensional space~\cite{KTC16}.
Hierarchical Rejection Sampling (HRS)~\cite{KTC16} is thus proposed that sequentially samples each dimension so that an unqualified candidate can be rejected before all dimensions are sampled. 
However, the rejection rate is still relatively high~\cite{KTC16}.
In this paper, we propose an informed sampling by an MCMC approach.
In order to obtained uniformly sampling on a manifold, MCMC provides an approach that samples from a high-dimensional uniform distribution subject to a manifold constraint.
Instead of rejecting samplings, original samples are purposefully driven onto a manifold.

One of the major difficulties of sampling the informed level set is that the shape of the cost function can change dramatically with the start and goal states. Because of this, there is not a principled way to sample from the informed set for each possible start and goal combination.  

Following the example of recent work in path planning algorithms such as CHOMP~\cite{RZBS09}, we propose to solve the problem of efficiently sampling uniformly in the level set by using Monte Carlo sampling methods. Monte Carlo sampling methods have been used in the statics and machine learning fields to sample from intractable posteriors. Monte Carlo methods have the desirable property that they are guaranteed to sample the true posterior as the number of steps goes toward infinity.


\begin{algorithm}
	\begin{algorithmic}[1]
		\STATE
   	\end{algorithmic}
	\caption{MCMC Informed Sampling}
	\label{alg:mcmc_informed_sampling}
\end{algorithm}
}

\section{Evaluation}
\label{sec:eval}

\subsection{Uniformity}

(Give a set of parameters)
We firstly need to propose a way of measuring uniformity.
Then we can apply it to compare MCMC approach with rejection sampling and hierarchical rejection sampling.
We are interested with
\begin{itemize}
	\item asymptotic behavior (assumption: MCMC converge to a close uniformity)
	\item convergence rate (assumption: MCMC converge faster)
\end{itemize}

\subsection{Efficiency}

The efficiency is shown by how the time is needed when the number of nodes are increased.
(Y: time spent) and (X: number of nodes).

\begin{figure}[tb]
	\centering
	\includegraphics[height = 4.5cm ]{fig/sampling_efficiency.pdf}
	\caption{
		\captionstyle
		Compare time efficiency of generating 1000 samplings.
	}
	\label{fig:compare_time_efficiency}
\end{figure}


\subsection{Integration}

We're going to test with RRT*, which falls into a informed RRT*.

We are going to show how the solution cost falls along time.
(Y: solution cost) and (X: time spent).

\begin{itemize}
\item DIMT (double integrator minimum time)
\item Dubins car
\item Airplane
\end{itemize}

\section{Conclusion}

What MCMC algorithm is the best to use?

Parameter tuning

Application to different state spaces

Parallel chains

estimating volume of informed space

%\appendix
\appendices
\section{HMC}
%base on https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/

In this section we provide background on Hamiltonian Monte Carlo (HMC). 
For a comprehensive description, see~\cite{N11},
for intuition, see~\cite{ZRDPKDBS13}.
Note that some of the notation in this section slightly differ from the ones used in the main body such as replacing the cost~$c$ with time~$t$. 

MCMC algorithms typically make use of random-walks to convergence to a target stationary distribution.
This is often inefficient, resulting in slow mixing\footnote{Informally, the \emph{mixing time} of a Markov chain is the time until it is ``close'' to its steady-state distribution.}.
HMC is an MCMC method that adopts physical-system dynamics (specifically Hamiltonian dynamics) rather than a probability distribution to produce the next state in the Markov chain. 

\subsection{Hamiltonian dynamics}
Hamiltonian dynamics describe an object's motion in terms of its position $x$ and momentum $q$ at some time $t$. 
Each position is associated with a potential energy $U(x)$.
Similarly, each momentum is associated with a kinetic energy $K(q)$.
The total energy (also called the Hamiltonian) of the system is constant and is defined as
$$
E(x, p) = U(x) + K(q).
$$
Hamiltonian dynamics describe how the kinetic and potential energies are converted one to the other as the object moves throughout a system in time. 
This is done via a set of differential equations known as the Hamiltonian equations:
$$
\frac{dx}{dt} = \frac{\partial E}{\partial q}, 
\hspace{5mm}
\frac{dq}{dt} = - \frac{\partial E}{\partial x}. $$
%
%The Hamiltonian equations describe an objectâ€™s motion in time, which is a continuous variable. In order to simulate Hamiltonian dynamics numerically, the Leapfrog method is typically used.

\subsection{From Hamiltonian dynamics to probability distribution}
To sample from a desired distribution~$\pi$ using HMC, we need to convert the distribution to an energy function and vice-versa. This is done using the concept of a canonical distribution from statistical mechanics~\cite{N11}. Given some energy function,~$E$, the canonical distribution over states has the Boltzman probability distribution $\pi = \frac{1}{Z}\exp (-E/T)$, where $Z$ is a normalizing constant and~$T$ is the temperature of the system.
Notice that in our setting
$$
\pi(x,q) 
\propto e^{-E(x,q)} 
= e^{-U(x)} \cdot e^{-K(q)}
\propto \pi(x) \cdot \pi(q).
$$
This means that the distributions $\pi(x)$ and $\pi(q)$ are independent. Therefore, we can use Hamiltonian dynamics to sample from the joint distribution over $q$ and $x$ and ignore the momentum contributions.
This is done by introducing auxiliary variables to represent the momentum and choosing any distribution from which to sample the momentum variables. A common choice is to use a zero-mean Normal distribution with unit variance.

\subsection{Hamiltonian Monte Carlo}
HMC uses Hamiltonian dynamics to explore the distribution $\pi(x)$ defined by $U(x)$. 
Starting at an initial state~$(x_0, q_0)$, 
(here, $q_0$ is the axillary variable used to simulate the momentum), Hamiltonian dynamics are simulated for a short time using the Leap-Frog method~\cite{S82}. 
Let $(x', q')$ be the state at the end of the simulation. 
This state is accepted using an update rule analogous to the Metropolis acceptance criterion~\cite{CG95}. 
Specifically, if the probability of $(x', q')$ after Hamiltonian dynamics is greater than probability of the state prior to the Hamiltonian dynamics, then $(x', q')$ is accepted.
Otherwise, $(x', q')$ is accepted randomly. 
If~$(x', q')$ is rejected, the next state of the Markov chain is set as the state at the previous time step. 

To explore the entire space, the dynamics are randomly perturbed. This is done by drawing a random momentum from $\pi(q)$ before running the dynamics prior to each sampling iteration $t$. Combining these steps, sampling random momentum, followed by Hamiltonian dynamics and Metropolis acceptance criterion defines the HMC algorithm.

\section{Gibbs sampling and Hit-and-run}
Gibbs sampling is a simple MCMC algorithm which samples each dimension (or variable) from its \emph{conditional distribution} with the remaining variables fixed to their current values.
Specifically, 



%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Old text that may be re-used
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ignore{
In order to take advantage of these powerful methods, we propose to reformulate the cost surface as a probability density. Specifically, we consider a Boltzmann distribution in order to formulate the cost function using flexible energy based methods. Because Monte Carlo methods only consider the ratio of the probabilities, we do not need to consider the problem of estimating the intractable partition function.

\begin{equation}
E_{\text{informed}}\left(t,t_0\right) = \beta_{\text{informed}} \text{sigm}\left(t_0 - t\right)
\end{equation}
where $\text{sigm}\left(\cdot\right)$ represents a tight sigmoid function that can be seen as a differentiable $\text{sign}\left(\cdot\right)$ function. $\beta_{\text{informed}}$ is some large number that controls how much we want to penalize states outside of the level set. In theory, the energy should be infinity (which represents zero probability). In practice, however, setting $\beta_{\text{informed}}$ to be some high number such as $100$ works well and is more stable.




Formally, we consider the probability distribution to be a Boltzmann distribution in the form of:

\begin{equation}
P\left(t,t_0\right) = \frac{1}{Z}\exp\left(-E\left(t,t_0\right)\right)
\end{equation}
where $Z$ is the intractable partition function that denotes the normalizing constant of the distribution and  $E\left(t,t_0\right)$ is the energy function which is dependent on the current best cost, $t_0$, and the cost of the path in question $t$.

The flexibility of the energy function is in the ability to specify desired or more probable states (by presenting lower energies) without the need to use a formal and restrictive model. Specifically for our problem, we defined the energy function to be:

\begin{equation}
E\left(t, t_0\right) = E_{\text{grad}}\left(t\right) + E_{\text{informed}}\left(t,t_0\right) + E_{\text{region}}\left(\mathbf{x}\right)
\end{equation}

\begin{equation}
E_{\text{grad}}\left(t\right) = \sigma\left(t\right)
\end{equation}
where $\sigma\left(\cdot\right)$ is some function that controls the flatness of the distribution.  Intuitively, this function prefers states with smaller costs. The flatter the function, the less pronounced this becomes. We experimented with using $\log\left(1 + \log\left(t\right)\right)$ and $\tanh\left(t\right)$. $\tanh$ resorts in a flatter distribution which may result in more uniform samples, but contains less gradient information for methods that use them such as Hamiltonian Monte Carlo.

\begin{equation}
E_{\text{informed}}\left(t,t_0\right) = \beta_{\text{informed}} \text{sigm}\left(t_0 - t\right)
\end{equation}
where $\text{sigm}\left(\cdot\right)$ represents a tight sigmoid function that can be seen as a differentiable $\text{sign}\left(\cdot\right)$ function. $\beta_{\text{informed}}$ is some large number that controls how much we want to penalize states outside of the level set. In theory, the energy should be infinity (which represents zero probability). In practice, however, setting $\beta_{\text{informed}}$ to be some high number such as $100$ works well and is more stable.

\begin{equation}
E_{\text{region}}\left(\mathbf{x}\right) = \beta_{\text{region}}||\mathbf{x}_{\text{limits}} - \mathbf{x}||_2
\end{equation}
where $||\mathbf{x}_{\text{limits}} - \mathbf{x}||_2$ represents the L2 distance between the current state $\mathbf{x}$ and the state limits $\mathbf{x}_{\text{limits}}$ and $\beta_{\text{region}}$ is some high number that controls how much to penalize states close to the limits. The same intuition for values of $\beta_{\text{region}}$ is the same as for $\beta_{\text{informed}}$.

}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliography{bibliography}
\bibliographystyle{IEEEtran}
%\bibliography{IEEEabrv,bibliography}


\end{document}
